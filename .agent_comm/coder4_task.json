{
  "agent_id": "coder4",
  "task_id": "task_4",
  "files": [
    {
      "name": "multi_agent_comm.py",
      "purpose": "Multi-agent communication",
      "priority": "medium"
    },
    {
      "name": "evaluation.py",
      "purpose": "Agent evaluation metrics",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2507.22640v1_Safe_Deployment_of_Offline_Reinforcement_Learning_",
    "project_type": "agent",
    "description": "Enhanced AI project based on stat.ML_2507.22640v1_Safe-Deployment-of-Offline-Reinforcement-Learning- with content analysis. Detected project type: agent (confidence score: 15 matches).",
    "key_algorithms": [
      "Value",
      "Facilitate",
      "Stabilise",
      "Promising",
      "Involves",
      "Reinforcement",
      "Theoretical",
      "Constrains",
      "Kinetic",
      "Safe"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: stat.ML_2507.22640v1_Safe-Deployment-of-Offline-Reinforcement-Learning-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nSafe Deployment of Offline Reinforcement Learning via Input\nConvex Action Correction\nAlex Durkin1, Jasper Stolte2, Matthew Jones2, Raghuraman Pitchumani3, Bei Li2,\nChristian Michler2, and Mehmet Mercang\u00a8 oz1\n1Department of Chemical Engineering, Imperial College London, SW7 2AZ, UK\n2Shell Information Technology International BV, 1031 HW Amsterdam, NL\n3Shell Global Solutions International BV, 1031 HW Amsterdam, NL\nAbstract\nOffline reinforcement learning (offline RL) offers a promising framework for developing control strategies in\nchemical process systems using historical data, without the risks or costs of online experimentation. This\nwork investigates the application of offline RL to the safe and efficient control of an exothermic polymerisa-\ntion continuous stirred-tank reactor. We introduce a Gymnasium-compatible simulation environment that\ncaptures the reactor\u2019s nonlinear dynamics, including reaction kinetics, energy balances, and operational con-\nstraints. The environment supports three industrially relevant scenarios: startup, grade change down, and\ngrade change up. It also includes reproducible offline datasets generated from proportional-integral con-\ntrollers with randomised tunings, providing a benchmark for evaluating offline RL algorithms in realistic\nprocess control tasks.\nWe assess behaviour cloning and implicit Q-learning as baseline algorithms, highlighting the challenges\noffline agents face, including steady-state offsets and degraded performance near setpoints. To address these\nissues, we propose a novel deployment-time safety layer that performs gradient-based action correction using\ninput convex neural networks (PICNNs) as learned cost models. The PICNN enables real-time, differen-\ntiable correction of policy actions by descending a convex, state-conditioned cost surface, without requiring\nretraining or environment interaction.\nExperimental results show that offline RL, particularly when combined with convex action correction,\ncan outperform traditional control approaches and maintain stability across all scenarios. These findings\ndemonstrate the feasibility of integrating offline RL with interpretable and safety-aware corrections for high-\nstakes chemical process control, and lay the groundwork for more reliable data-driven automation in industrial\nsystems.\nKeywords\nOffline Reinforcement Learning, Process Control, Safety, Convex Neural Networks\n1 Introduction\nThe application of reinforcement learning (RL, [1]) to industrial process control holds promise for improving\nefficiency, adaptability, and long-term performance. However, real-world deployment of RL policies in such\nsettings remains rare, particularly in high-stakes domains like chemical processing, power generation, and\nautonomous manufacturing, due to strict safety and stability requirements [2]. This concern is heightened\nin the context of offline RL, where policies are trained entirely from historical data with no opportunity for\nonline trial-and-error [3]. While this mitigates the risk of unsafe exploration during training, it also introduces\n1arXiv:2507.22640v1  [eess.SY]  30 Jul 2025\n\n--- Page 2 ---\nthe challenge of verifying and correcting policy behaviour at deployment time, when unseen disturbances or\ndistributional shifts can lead to failure.\nIn industrial systems, a poorly chosen control action can have serious consequences, including damaging\nequipment, violating regulatory constraints, or causing cascading system instability. Offline-trained RL\nagents, optimised for historical performance, may produce unsafe or unstable actions when operating in\nregions underrepresented in the training data. Therefore, ensuring safety at deployment becomes a critical\nbottleneck in applying RL to real-world process control.\nPolymerisation grade transitions are critical but challenging operations in continuous reactors, requiring\nchanges in feed composition, temperature, and other conditions to switch from one polymer product to an-\nother [4]. These transitions are nonlinear, constrained, and economically sensitive: off-specification material\ngenerated during the switch is wasteful, while long transition times reduce throughput [5, 6]. Managing grade\nchanges efficiently demands controllers that balance safety, product quality, and operational speed\u2014making\nthem an ideal testbed for intelligent control methods such as RL.\nControl theory offers powerful tools to address this challenge. In particular, the concepts of asymptotic\nstability and regions of attraction are foundational in guaranteeing that a control policy not only keeps the\nsystem bounded but also drives it to a desired operating regime [7]. These properties are often certified using\nLyapunov functions, which provide a scalar energy-like measure that decreases over time under the system\ndynamics, ensuring stability and safe recoverability from perturbations [8].\nInspired by this perspective, our approach introduces a deployment-time safety mechanism for offline RL\nagents using partially input convex neural networks (PICNNs, [9]) as learned cost models. These models are\nconstructed to be convex in the control actions and expressive over the state, enabling efficient, gradient-based\ncorrection of unsafe actions. When the offline RL policy proposes an action that may compromise stability\nor violate safety constraints, the PICNN model provides a locally optimal, safe correction by descending its\nconvex cost surface. Although not explicitly trained as Lyapunov functions, the structure of these models\nensures they behave like a learned control energy landscape that discourages unsafe actions and guides the\nsystem toward safe operating regions.\nThis architecture is particularly well-suited for process control applications, where control inputs must\nbe interpretable, verifiable, and responsive to safety constraints. Our method acts as a safety layer over\narbitrary offline policies, enabling stable and conservative control corrections in real time without requiring\nmodifications to the underlying policy or additional interaction with the environment.\nThis paper proceeds with a summary of the related work in literature followed by a summary of our\ncontributions. Section 2 provides some background on offline RL and PICNNs for safe control. Section 3\ndescribes the benchmark environment and datasets used for evaluation. Section 4 details the offline training\nof RL agents, including behaviour cloning and implicit Q-learning as well as the PICNN cost model. Section 5\noutlines our novel online action correction algorithm. Section 6 presents the experimental results, comparing\nthe performance of offline RL agents with and without PICNN-based action correction. Finally, Section 7\nconcludes with a discussion of the findings and future work.\n1.1 Related Work\nA range of methods has been proposed to incorporate safety into RL, especially in the context of continuous\ncontrol. A prominent direction is the use of Lyapunov-based techniques to constrain policy updates or\nverify system stability. For example, [10] introduced a model-based RL framework that leverages Lyapunov\nfunctions to guarantee safe learning. Their method models uncertainty in the dynamics using Gaussian\nprocesses to estimate dynamics and enforce stability through Lyapunov constraints. Other efforts apply\ncontrol barrier functions (CBFs) to ensure constraint satisfaction through quadratic programming [11], and\nsome integrate CBFs with control Lyapunov functions to achieve safety and stability simultaneously [12].\nThese techniques offer strong guarantees but often assume access to accurate dynamics models and are\nprimarily applied during training or online control.\nAnother branch of safe RL research focuses on constrained policy optimisation. Constrained policy op-\ntimisation [13] ensures monotonic policy improvement while respecting safety constraints in expectation.\nOther works adopt primal-dual or Lagrangian methods to dynamically balance reward maximisation and\nconstraint enforcement [14]. In the offline RL setting, safety becomes even more critical due to the absence\nof exploration. Conservative Q-learning [15], implicit Q-learning [16], and model-based offline policy optimi-\n2\n\n--- Page 3 ---\nsation [17] address distributional shift and unsafe extrapolation by constraining policies to remain close to\nthe behaviour policy or trusted regions of the dataset. While these methods reduce the risk of catastrophic\nactions during inference, they lack mechanisms to actively correct unsafe outputs at deployment.\nOther approaches learn explicit safety critics or constraint models. For example, Lyapunov-based actor-\ncritic methods [18] train neural approximations of Lyapunov functions to evaluate and constrain policy\nupdates. Risk-sensitive methods model quantiles or variance of returns to encode safety preferences [19].\nAction correction techniques such as those in [14] modify unsafe actions using learned penalty gradients,\nwhile adaptive shielding mechanisms preemptively reject or reroute unsafe behaviours using external models\n[20]. These methods provide a degree of runtime robustness but often require constraint supervision or\nmultiple value functions, increasing complexity.\nTo improve action selection reliability, several works introduce architectural biases into RL function\napproximators. It has been demonstrated that PICNNs can be used in the context of RL to model the\nnegative action-value function, as a convex function over the action space [9]. This enables efficient inference\nof optimal actions through convex optimisation, making the architecture particularly suitable for structured\ndecision-making tasks. The convex structure ensures global optima and promotes stable action selection, a\ndesirable property for safe control. Follow-up work has extended PICNNs to learn convex control models\n[21] or embed differentiable convex solvers into policy networks [22]. Related architectures like OptNet [23]\nand differentiable quadratic programming layers [22] allow NNs to output solutions to embedded convex\nprograms, opening new paths for safe and structured control.\nFinally, applying RL to real-world process control systems introduces unique demands that go beyond\ntypical benchmark settings. These systems are highly sensitive to even brief periods of instability, and safety\nconstraints are often implicit, non-differentiable, or time-varying. In industrial practice, model predictive\ncontrol (MPC) is the prevailing strategy for handling such complexities. For example, a polyethylene pro-\nducer reported reducing transition times by 25\u201350 %, cutting off-spec product, and increasing throughput by\n7 % using an MPC system combined with real-time optimisation [4]. Hybrid approaches that fuse mechanis-\ntic models with data-driven components, such as NNs augmenting mass and energy balances, have also been\ndeployed to provide virtual sensors and improve predictive accuracy during polymer grade transitions [24].\nMore recently, RL agents trained in simulation have shown performance on par with tuned nonlinear MPC\ncontrollers for managing grade changes, highlighting their potential for data-driven process optimisation [25].\nSeveral benchmark environments have been developed to support safe and realistic RL research in these\ndomains. The Industrial Benchmark [26], Safety Gym [27], and PC-Gym [28] all simulate control tasks with\nrealistic dynamics, constraints, and disturbances. Prior work emphasises the importance of deployment-\ntime guarantees, real-time corrective mechanisms, and interpretable architectures to bridge the gap between\nsimulation and operational deployment [29].\n1.2 Our Contributions\nBuilding on prior work in Lyapunov-based safe control and convex neural architectures, we introduce a novel\ndeployment-time safety mechanism tailored for offline RL policies in industrial process control environments.\nRather than relying on handcrafted Lyapunov functions or using PICNNs to approximate Q-values during\ntraining, we reinterpret PICNNs as state-conditioned cost surfaces that guide real-time correction of unsafe\nactions.\nOur approach constructs a convex energy landscape over the action space using a PICNN, which is\nexpressive in state and convex in action. At deployment, when an offline policy proposes a control input, the\nPICNN provides a corrective gradient that descends the learned cost surface, nudging the action toward safer,\nmore stable alternatives. This gradient-based filtering mechanism avoids the need for explicit constraint sets\nor online exploration and can be applied to any offline policy without retraining or modifying its structure.\nTo evaluate this approach, we develop a Gymnasium-compatible benchmark environment that models\na continuous stirred-tank reactor undergoing exothermic polymerisation. The environment captures the\nnonlinear dynamics, energy balances, and safety-relevant operating constraints typical of industrial grade\ntransition scenarios, providing a realistic testbed for data-driven control strategies. Offline datasets are\ngenerated using PI controllers with diverse tuning parameters to simulate suboptimal operational behaviour.\nTo our knowledge, this is the first method to use PICNNs as learned safety surrogates for deployment-\ntime action correction in high-stakes control domains. Our system functions as a lightweight safety layer\n3\n\n--- Page 4 ---\nthat is interpretable, responsive, and compatible with real-time constraints, offering a practical pathway to\nbridge the gap between offline RL and safety-critical real-world deployment.\n2 Background\nRL is the field within machine learning in which an agent learns to optimise its behaviour in an environment\nthrough interaction [1]. The key advantage of RL is that it can learn a control policy which deals with\nthe trade off between greediness and patience to chase an even greater expected reward in the future. The\ntheoretical framework underpinning RL is the Markov decision process (MDP).\n2.1 Markov decision process\nThe RL problem can be formalised in the context of determining a policy \u03c0(a|s) to maximise the cummulative\nreward of a MDP given by \u27e8S,A, p0(s), p(s\u2032|s, a), r(s, a), \u03b3\u27e9, where Sis the state space, Ais the action space,\np0(s) is the initial state distribution, p(s\u2032|s, a) is the transition function, r(s, a) is the reward function, and \u03b3\nis the discount factor where more importance is placed on future rewards as \u03b3\u21921. A common strategy to\nsolve the MDP involves approximating the state\u2013action value function, Q(s, a), referred to as the Q-function,\nwhich is defined as the expected cumulative reward of taking action ain state sand following the policy \u03c0.\nThe optimal policy, \u03c0\u2217, is then given by:\n\u03c0\u2217(a|s) = arg max\naQ(s, a) (1)\nProcess control applications involve continuous state\u2013action spaces, with the value function control poli-\ncies commonly modeled using NNs. Three distinct classifications of RL algorithms can be identified as\ndemonstrated in figure 1. In the usual case the agent can iterate towards a solution, collecting feedback\nfrom every interaction cycle. In off-policy algorithms a buffer of historic observations is preserved to increase\nsample efficiency and prevent convergence to local optima. Critically, in off-policy algorithms the agent still\ncollects feedback on its current policy through rollouts of suboptimal agents during training. In chemical\nprocess control, deployment of partially trained agents on real production plants is infeasible for safety and\neconomic reasons. Thus, we can either train an agent on a simulated process to subsequently deploy it on\nthe real plant, or train an agent on data taken from the real process.\nFigure 1: Three different classes of reinforcement learning (taken from [30]). Offline reinforcement learning\nis a special case of off-policy learning leveraging historic data only.\nUsing a simulator to train agents has several drawbacks. The most important one being that performance\nwill suffer from the sim-to-real gap as realistic behaviour inherent to chemical processes is difficult to capture\nin the simulator. High fidelity dynamic simulation models are typically expensive to develop and maintain\nmeaning they will not be available for most processes. These models are also slow leading to high compute\nrequirements to train a policy. For these reasons, the authors propose training on historic process data\n(offline RL) as the preferred approach.\n4\n\n--- Page 5 ---\n2.2 Offline RL\nIn offline RL [31, 32] a buffer of historic observations is used to train a policy for the agent. In this setting\nthe data was collected under an unknown behaviour policy \u03c0\u03b2, which could be any combination of manual\noperation with supervisory control. The objective is to learn a safe and robust control policy from this data\nwithout further environment interaction.\nThe key challenge with offline RL is distributional shift [33], which can be conceptually understood as the\ndifficulty to accurately estimate the value of state\u2013action combinations that were not observed in the historic\ndata. To illustrate, consider the temporal difference loss to minimise the Bellman error for the Q-function\nas given below:\nL(\u03b8) =ED\u0014\u0010\nr+\u03b3max\na\u2032Q\u02c6\u03b8(s\u2032, a\u2032)\u2212Q\u03b8(s, a)\u00112\u0015\n(2)\nwhere Dis the dataset of (state, action, next state) observations, Q\u03b8is the parameterised Q-function\nprediction for the current state\u2013action pair, and r+\u03b3max a\u2032Q\u02c6\u03b8is the Bellman equation for calculating the\ntarget Q-value as the sum of the immediate reward and the discounted maximum expected future rewards.\nThe challenge with distributional shift is that the loss in equation 2 contains a term Q\u02c6\u03b8(s\u2032, a\u2032) which\nrequires evaluating the Q-function at state\u2013action combinations that are not present in the data. Even worse,\nthis Q-function evaluation is maximised over all possible future actions leading to gross value overestimation,\nlearning policies that exploit this non-existing value.\nTypical strategies to stabilise learning as employed in online RL include double Q-learning [34] and using\ntarget networks [35] or simply using vast amounts of training data. These strategies can only slightly help\nreduce the magnitude of the value overestimation but do not fundamentally solve the problem. Since this\nissue was clearly articulated in [33] in 2019 the field of offline RL gathered momentum and many strategies\nhave been devised to develop policies with more robustness towards distributional shift. Algorithms such\nas conservative Q-learning [15], IQL [16], and behaviour-regularised actor-critic [36] mitigate distributional\nshift by constraining policy updates toward the behaviour policy or learning conservative value estimates.\n2.3 Behaviour cloning\nBC is a straightforward imitation learning approach in which a policy is trained to directly mimic the actions\ntaken by an expert or behaviour policy using supervised learning. BC ignores the reward signal entirely and\nonly attempts to mimic the actions from the dataset as closely as possible. Formally, the BC loss function\nis given by equation 3\nLBC(\u03d5) =E(s,a)\u223cD[\u2212log\u03c0\u03d5(a|s)] (3)\nwhere Dis the offline dataset of state\u2013action pairs, and \u03d5denotes the parameters of the policy network.\nIf the dataset has good coverage of the state space and contains high-quality examples, BC can be expected\nto give good results. In the face of limited and/or subobtimal data BC is likely to give poor performance.\nRegardless, it should always be considered as a baseline in offline RL problems.\n2.4 Implicit Q-learning\nImplicit Q-learning (IQL) [16] is an offline RL approach that avoids the distributional shift problem by\nnever querying the target Q-function for state\u2013action combinations that are not in the dataset. This fully\nin-sample learning depends on an approximation of the Q-value of the next state\u2013action combination by\na state-value function. IQL leverages expectile regression to predict an upper expectile of the state-value\nfunction with respect to the action distribution:\nLV(\u03c8) =E(s,a)\u223cD\u0002\nL\u03c4\n2\u0000\nQ\u02c6\u03b8(s, a)\u2212V\u03c8(s)\u0001\u0003\n(4)\nwhere L\u03c4\n2(u) =|\u03c4\u2212 1(u < 0)|u2is the expectile loss function, which is a generalisation of the mean\nsquared error loss which enables asymmetric weightings to favour larger expectiles. If \u03c4is equal to 0 .5, the\nexpectile regression is identical to the normal MSE. As \u03c4is increased towards 1 the value no longer learns\nthe average value of a given state over the action distribution but approaches the maximum value supported\n5\n\n--- Page 6 ---\nby the actions in the data. This estimated value function is then used to backup into the Q-function update\ninstead of Q\u02c6\u03b8(s\u2032, a\u2032):\nLQ(\u03b8) =E(s,a,s\u2032)\u223cDh\n(r(s, a) +\u03b3V\u03c8(s\u2032)\u2212Q\u03b8(s, a))2i\n(5)\nOnce an estimated Q-value function is learned, the policy maximising the expected value is extracted\nusing AWR [37], which also only evaluates the Q-function at state\u2013action combinations that are present in\nthe dataset:\nL\u03c0(\u03d5) =EDh\ne\u03b2(Q\u02c6\u03b8\u2212V\u03c8)log\u03c0\u03d5(a|s)i\n(6)\nwhere \u03b2is an inverse temperature parameter which controls the trade-off between maximising the Q-\nfunction ( \u03b2\u2192 \u221e ) and staying close to the behaviour policy ( \u03b2\u21920) thereby supporting a spectrum of risk\nsensitivities. AWR is based on a forward KL divergence regularisation pulling the policy towards actions\nobserved in the data, pulling more strongly towards actions of higher estimated value.\n2.5 Lyapunov-inspired safety and PICNNs\nIn classical control theory, one of the foundational tools for certifying stability and safety is the Lyapunov\nfunction. This is a scalar-valued function that decreases along system trajectories and provides a formal\nguarantee that the system will converge to a safe operating regime [8]. While constructing Lyapunov functions\nanalytically for complex nonlinear systems is difficult, recent work has explored learning Lyapunov-like\nfunctions directly from data using structured NNs.\nA promising approach in this direction is the use of PICNNs [9], which are feedforward architectures\nspecifically designed to be convex in a subset of their inputs. Convexity plays a key role in optimisation\nand control, ensuring global optima and tractable inference. Standard NNs lack this property, limiting\ntheir applicability in structured prediction, inverse optimisation, and safe control tasks where convexity is\ndesirable. PICNNs overcome this by constraining network weights and activations: all weights on the convex\ninput path are non-negative, activation functions are convex and non-decreasing (e.g., ReLU, Softplus). To\nhandle contextual variables such as system state, PICNNs also allow an additional non-convex input s, while\npreserving convexity in the decision variable a. The hidden layers of a PICNN are structured as:\nzi+1=\u03d5(W(i)\nzzi+W(i)\naa+ui(s)) (7)\nwhere ais the convex input and ziare hidden activations, and ui(s) is an unconstrained transformation\nof the state. This architecture is shown in Figure 2\nFigure 2: PICNN architecture adapted from [9]. The network is convex in the action input awhile being\nexpressive over the state s.\nIn our work, we reinterpret PICNNs as deployment-time safety filters: by constructing a convex cost\nlandscape over the action space, conditioned on the current state, we use the gradient of the PICNN to\nadjust potentially unsafe actions proposed by offline RL policies. This leverages the convexity of PICNNs for\ntractable correction, while preserving expressivity over the system state\u2014a crucial requirement for safety-\ncritical industrial control tasks. This approach can be considered as modelling a learned Lyapunov-like cost\nfunction upon which gradient descent is performed at test time to decrease the cost and guide the system\ntowards safer operating regions over the course of the rollout.\n6\n\n--- Page 7 ---\n3 Benchmark environment\nA good benchmark environment with corresponding datasets is required to explore RL for process control\napplications. Whilst [38] and [28] recently proposed several benchmark problems for this domain, their\ncase studies do not capture one of the most practically relevant and challenging scenarios: product grade\ntransitions. These transitions are particularly well-suited to RL, as they are often executed under close\noperator supervision and frequently involve manual interventions leading to rich historical datasets. In this\npaper we introduce a new benchmark environment for product grade transitions, derived from a similar\nsuspended polymerisation reactor example from literature [39].\nA simulation environment was developed based on a continuous stirred tank reactor (CSTR) undergoing\nan exothermic free-radical polymerisation reaction. The environment was implemented using the Gym-\nnasium API to ensure compatibility with standard RL workflows. It incorporates detailed material and\nenergy balances to realistically capture the nonlinear dynamics and control challenges inherent to industrial\npolymerisation processes.\n3.1 Reactor model\nA schematic of the CSTR is shown in Figure 3. The CSTR is fed with a solvent (S) containing monomer\nspecies (M) and an initiator species (I) at temperature Tin. In the CSTR, the monomer and initiator react\nto form a polymer product (P) via a suspended lumped radical (R) mechanism. The CSTR outlet flowrate\nis assumed to be equal to the total inlet flowrate, maintaining a constant liquid filled reactor volume (1\nm3). Perfect mixing is assumed along with constant fluid density (1000 kg/m3). Any gas phase effects and\nchanges in viscosity due to polymer build-up are neglected.\nFigure 3: Schematic of the polymerisation CSTR environment. Control actions include the initiator feed rate\n(FM,in) and cooling jacket temperature TC. The reactor states include monomer concentration ( CM), initiator\nconcentration ( CI), radical concentration ( CR), polymer concentration ( CP), and reactor temperature ( T),\nwhere the latter two are also the controlled variables.\nThe reaction pathway includes thermal auto-decomposition of the initiator to generate radicals (8),\npropagation of monomer with radicals to form polymer chains (9), and bimolecular termination of radicals\n(10). In this mechanism, all radical species (including free radical and propagating chain radicals) are\nrepresented with a single concentration variable for the lumped radicals, CR. This approximation simplifies\nthe kinetic model by avoiding the need to track individual chain lengths. The reaction set (8\u201310) describes\nthe chemical reactions.\n7\n\n--- Page 8 ---\nInitiation: Iki\u2212 \u2212 \u2192 2 R\u00b7 (8)\nPropagation: M + R \u00b7kp\u2212 \u2212 \u2192 P\u00b7 (9)\nTermination: R \u00b7+ R\u00b7kt\u2212 \u2212 \u2192 P (10)\nReaction kinetics are modeled using temperature-dependent Arrhenius expressions with constant pre-\nexponential factors and activation energies given in Table 1. All reactions are assumed to be kinetically\ncontrolled with no mass transfer limitations.\nk=Aexp\u0012\n\u2212E\nRT\u0013\n(11)\nThe reaction rates are defined as follows:\nInitiation: ri=kiCI (12)\nPropagation: rp=kpCMCR (13)\nTermination: rt=ktC2\nR (14)\n3.2 Dynamics and numerical integration\nThe reactor dynamics are governed by a system of coupled ordinary differential equations (15\u201319) using the\nparameters in Table 1. Heat effects are explicitly modelled, with an exothermic heat of reaction and thermal\nexchange with a cooling jacket. These are solved at each simulation step using SciPy\u2019s solveivp function\nwith a stiff solver (BDF). The integration time step is set to 30 minutes to reflect typical control intervals for\nlarge-scale batch or semi-batch reactors. The simulation is terminated after 100 hours of operation unless\notherwise specified.\ndCM\ndt=F\nV\u0000\nCin\nM\u2212CM\u0001\n\u2212rp (15)\ndCI\ndt=F\nV\u0000\nCin\nI\u2212CI\u0001\n\u2212ri (16)\ndCR\ndt=F\nV\u0000\nCin\nR\u2212CR\u0001\n+ 2ri\u22122rt (17)\ndCP\ndt=F\nV\u0000\nCin\nP\u2212CP\u0001\n+rp (18)\ndT\ndt=F\nV(Tf\u2212T)\u2212UA(T\u2212Tc)\n\u03c1CpV\u2212rp\u2206Hrxn\n\u03c1Cp(19)\nTable 1 gives the parameters used to construct the polymerisation reactor environment. The reaction is\ninspired by the polymerisation of vinyl acetate in benzene as presented in [39], with some simplifications and\nrounding of parameters.\n3.3 RL environment implementation\nThe reactor model is embedded in a Gymnasium environment and key environment parameters are listed in\ntable 2.\nRL agents interact with the environment via two continuous control actions: the initiator feed rate\n(kg/h) and the cooling jacket temperature (K). These are bounded within operationally realistic ranges:\n0.0\u20132.5 kg/h for the initiator and 300\u2013350 K for the coolant temperature. The observation space includes\nthree measured process variables: monomer concentration, polymer concentration, and temperature of the\noutlet. Note that this leaves important simulation state variables such as initiator and radicals concentration\ninvisible to the agent. Noise is added on the process variables measurements to add complexity and realism.\n8\n\n--- Page 9 ---\nTable 1: Summary of reaction/reactor parameters used in the PolyCSTR environment\nParameter Description Value Unit\nPhysical Constants\nR Gas constant 8.314 J/mol/K\n\u03c1 Density 1000 kg/m3\nCp Heat capacity 2000 J/kg/K\nU Heat transfer coefficient 500 J/s/K\n\u2206Hrxn Heat of reaction -100 kJ/mol\nReactor Configuration\nV Reactor volume 1.0 m3\nFS Feed solvent rate 80 kg/h\nFM Feed monomer rate 100 kg/h\nTf Feed temperature 350 K\nKinetics\nAinit Pre-exponential factor (initiation) 1 \u00d71091/s\nEinit Activation energy (initiation) 125 kJ/mol\nAprop Pre-exponential factor (propagation) 4 \u00d7104m3/mol/s\nEprop Activation energy (propagation) 25 kJ/mol\nAterm Pre-exponential factor (termination) 1 \u00d7106m3/mol/s\nEterm Activation energy (termination) 15 kJ/mol\nControl Inputs\nFI Initiator feed rate 0.0 - 2.5 kg/h\nTc Coolant temperature 300 - 355 K\nSimulation Settings\n\u2206t Time step 0.5 h\nTmax Max simulation time 100 h\nBoth temperature and polymerisation have setpoints, and the observation space is augmented with error\n(2x) and error rate of change (2x) variables making for a total of 7 process related observations. To facilitate\nlearning directionally correct strategies, a delta-action approach is taken. Thus, the state space is further\naugmented with the current absolute values of the action variables, bringing the total observation space\nto 9 continuous variables. The action space is given by the change in initiator feed and change in coolant\ntemperature with respect to the current situation.\n3.3.1 Rewards\nTo guide the agent toward effective and safe grade transitions, we design a reward function that encourages\nprogress toward the desired setpoint while penalising unsafe behaviour. The primary reward signal is based on\nerror reduction: the agent is rewarded for decreasing the absolute deviation in concentration and temperature\nfrom their respective targets between time steps. This difference-in-error formulation incentivises consistent\nimprovement rather than absolute accuracy at any single step. To further stabilise control near the target,\nwe introduce a setpoint proximity bonus: if both concentration and temperature remain within predefined\nthresholds (2.0 and 1.0 units, respectively), the agent receives an additional reward scaled linearly by how\nclose the state is to the setpoint. This encourages the agent not only to reach the desired operating region\nbut to remain there with minimal deviation. Finally, to ensure safety, a hard penalty of \u22121000 is applied\nif the reactor temperature drops below \u221250\u25e6C, representing a thermal runaway condition. This structure\nbalances short-term progress, long-term stability, and safety-critical constraints\u2014key priorities in industrial\nprocess control. The reward function is hereby defined as:\nrt=(\n\u22121000, ifeT<\u221250 (thermal runaway)\n\u2206et+bt,otherwise(20)\n9\n\n--- Page 10 ---\nTable 2: Polymerisation reactor Gymnasium environment characteristics.\nVariable Description Unit\nStatesCM Monomer concentration kg/m3\nCP Polymer concentration kg/m3\nT Reactor temperature K\neCP Polymer SP tracking error kg/m3\neT Temperature SP tracking error K\n\u2206eCP Change in polymer SP tracking error kg/m3\n\u2206eT Change in temperature SP tracking error K\nFinit Current initiator feed rate kg/h\nTc Current coolant temperature K\nActions\u2206Finit Change in initiator feed rate kg/h\n\u2206Tc Change in coolant temperature K\nReward r(s, a) Setpoint tracking with stability bonus (Eq. 20) \u2013\nwhere:\n\u2206et=\u0000\n|et\u22121\nCP|+|et\u22121\nT|\u0001\n\u2212\u0000\n|et\nCP|+|et\nT|\u0001\n(error reduction) (21)\nbt=\uf8f1\n\uf8f2\n\uf8f310\u00b7\u0012\n1\u2212max\u0012\n|et\nCP|\n\u03c4CP,|et\nT|\n\u03c4T\u0013\u0013\n,if|et\nCP|< \u03c4CPand|et\nT|< \u03c4T\n0, otherwise(22)\nwith thresholds: \u03c4CP= 2.0 kg/m3,\u03c4T= 1.0 K\n3.3.2 Costs\nIn addition to the reward signal used for policy optimisation, we define a separate cost function based on\nthe sum of absolute setpoint tracking errors:\nct=|et\nCP|+|et\nT| (23)\nThis cost serves a distinct role from the reward. While the reward encourages immediate progress and\nincentivises reaching the setpoint region, the cost is designed to be always non-negative, enabling its use as\na proxy for system stability\u2014analogous to a Lyapunov function. In our architecture, this cost function is\nleveraged by the safety filter to perform deployment-time correction: the PICNN models a state-conditioned\nsurrogate cost landscape, and its gradient is used to adjust unsafe actions proposed by the offline RL policy\nin a direction that locally reduces this cost. Defining the cost separately allows the reward to remain\nflexible (e.g., for shaping learning signals), while the cost is tightly aligned with operational safety and\nlong-term control objectives. More generally, in industrial process control settings, such costs could include\nnot only setpoint deviations but also absolute constraint violations, such as pressure limits, flow bounds, or\ntemperature thresholds.\n3.4 Control scenarios\nThree scenarios are considered for the polymerisation reactor control.\n1.Startup . In startup mode the reactor is initialised at equilibrium from a temperature and composition\nview, but without any initiator flow. Without initiator, there are no radicals and there can be no\npolymerisation. The objective in this scenario is to go from a polymer concentration of 0 kg/m3to\n100 kg/m3, while maintaining reactor temperature at 350 K.\n10\n\n--- Page 11 ---\n2.Grade change down . In this scenario the reactor the starting point is equilibrium operation at 350\nK and 100 kg/m3of polymer. The objective is to change the operation to a new setpoint of 355 K and\n90 kg/m3of polymer. At the higher temperature with lower conversion the chain length distribution\nwill skew towards the shorter chains yielding a lower density polymer.\n3.Grade change up : This is the reverse of scenario 2, moving back towards 350 K and 100 kg/m3\npolymer.\nTo generate offline data for training and evaluation, PI controllers were implemented to regulate polymer\nconcentration and reactor temperature to their predefined setpoints through manipulation of the initiator\nfeed flow and cooling jacket temperature respectively. The initialisation is such that there is no p-action\nkick from the setpoint change. The PI algorithm gain parameters (K pand K i) are uniformly sampled from\na factor 10 range, giving variation between rollouts. Anti-windup logic is applied to prevent further integral\nterm accumulation when actuators are saturated.\nEach scenario is rolled out 100 times in simulation, for a duration of 100 hours with a 30 min sample\nfrequency. This creates a dataset of 20,000 samples per scenario across 100 different PI tuning parameter\ncombinations. Note that the reaction kinetics and temperature controller strength are such that there is a\nreal risk of a runaway reaction if the temperature get too high in the presence of sufficient initiator, creating\nan interesting control challenge. With more conservative controller settings, the reactor won\u2019t reach the\nsetpoint within the 100 hours of simulation time. With more aggressive settings the reaction may runaway\nuncontrollably in the more challenging startup and grade change down scenarios. By comparison, the grade\nchange up is comparatively simpler as the temperature is reduced rather than increased. Thermal runaway\nwould occur at a reactor temperature of approximately 365 K, depending on the exact concentration of\nmonomer and initiator.\nThe challenge for the RL agent is to distill a policy combining the best parts of all the trajectories\nsupported by the data. Aggressive where it can be, careful where it needs to be. The non-linear nature\nof reinforcement learning agents allows for variation of the strategy with respect to reactor conditions that\ncannot be captured in a fixed PI controller strategy.\n4 Offline learning\n4.1 Behaviour cloning\nWe trained a BC agent as a supervised learning baseline using a dataset of state-action pairs collected from\nthe PolyCSTR environment. The policy is represented by a multilayer perceptron (MLP) with two hidden\nlayers, each containing 256 units and ReLU activations. The network maps observed states to actions under\na Gaussian policy, where the mean and standard deviation are predicted by the network. The policy was\noptimised using the Adam optimiser with a learning rate of 0 .0003. Training was conducted for 200 epochs\nwith a batch size of 512, using the full offline dataset as a replay buffer. The output actions were normalized\nand scaled to lie within the bounds of [ \u22121,+1], with a maximum action magnitude set to 1.0. This BC agent\nserves as a strong supervised learning baseline, capturing expert-like behavior directly from demonstration\ndata without any explicit RL objective.\n4.2 IQL agent training\nWe implemented IQL agents using NN architectures for the Q-function, value function, and policy networks.\nEach of these networks was parameterised as a fully connected multilayer perceptron (MLP) detailed below.\n\u2022Q-network: The Q-function was modeled using a feedforward MLP with 2 hidden layers, each contain-\ning 256 units and using ReLU activation functions. The output layer produced a single scalar Q-value\nestimate for each state\u2013action pair.\n\u2022Value network: The state-value function was modeled similarly with 2 hidden layers and 256 units per\nlayer, also using ReLU activations. The output was a scalar estimate of the expected return from a\ngiven state.\n11\n\n--- Page 12 ---\n\u2022Policy network: The policy was represented as a stochastic policy with a Gaussian head, using an MLP\nwith 2 hidden layers, 256 units each, and ReLU activations. The output layer produced the mean and\nlog standard deviation of the action distribution.\nThe IQL networks were trained on the offline polymerisation CSTR datasets. Training was conducted\nusing mini-batches of size 512. The temporal discount factor was set to 0.9. Target networks were updated\nusing a soft update rule with a coefficient of 0.05. The expectile parameter \u03c4used in the expectile regression\nloss for the value function was set to 0.9. The temperature \u03b2governing the weight of Q-values in policy\nextraction was set to 5. Training was performed over 2000 epochs using a replay buffer containing 20100\ntransitions. Learning rate decay was implemented using a cosine annealing schedule.\n4.3 Learning PICNN cost models\nA key challenge in deploying offline RL policies in real-world environments lies in ensuring practical safety\nwhilst ensuring policy competence. One promising strategy involves learning a cost function that captures\nconstraint violations, and using its gradient to refine the policy\u2019s action outputs at test time. However,\nstandard NN cost models are typically non-convex in the action space, leading to unreliable gradients during\ncorrection which can be particulalry problematic in safety-critical domains.\nIn this work, we propose to address this issue by leveraging PICNNs as cost models. The PICNNs were\ntrained to ensure convexity in the action space, whilst ensuring full expressivity over the state space. This\nstructural property ensures that the learned cost function c(s, a) is convex in afor every state s, enabling\nstable and predictable gradient-based corrections. To the best of our knowledge, this is the first application of\nPICNNs as cost models for real-time action refinement in safe RL deployment. we demonstrate that enforcing\naction-convexity in the cost model leads to smoother cost landscapes, more reliable descent directions, and\nulitimately, safer policy behaviour during deployment.\nPICNN cost models were trained and compared to standard NNs. For comparison, the standard NN and\nthe PICNN cost models were trained using the same paramters and architecture, with the only difference\nbeing that the PICNN cost model was trained to ensure convexity in the action space.\nTable 3: PICNN and standard NN cost model training parameters.\nParameter Value\nNo. hidden layers 2\nNo. hidden nodes 64\nActivation function Softplus\nMax no. epochs 1000\nMinibatch size 512\nLearning rate 0.001\nOptimiser Adam\nLoss function MSE\n5 Online action correction\nWe propose a deployment-time safety mechanism that refines actions from an offline policy using a learned\ncost model. Specifically, we model a state-conditioned cost function \u02c6 c\u03d5(s, a) using a PICNN, which is\nguaranteed to be convex in the action aand expressive in the state s. Given that convexity in aensures\na unique global minimum, we can apply principled optimisation methods to correct unsafe or suboptimal\nactions proposed by the policy.\nLeta0=\u03c0(s) be the original action suggested by the trained policy. The goal is to compute a corrected\naction a\u2217that reduces the cost \u02c6 c\u03d5(s, a) while remaining computationally feasible for real-time control.\nGradient Descent Update. The simplest update rule uses the gradient of the cost with respect to the\naction:\nak+1=ak\u2212\u03b7\u2207a\u02c6c\u03d5(s, ak) (24)\n12\n\n--- Page 13 ---\nwhere \u03b7 >0 is a fixed or adaptive step size. This update moves in the direction of steepest descent and is\nguaranteed to reduce the cost under convexity assumptions.\nNewton Step Update. When the second derivative (Hessian) is available and the cost function is twice-\ndifferentiable in a, a more efficient update is given by:\nak+1=ak\u2212\u0000\n\u22072\na\u02c6c\u03d5(s, ak)\u0001\u22121\u2207a\u02c6c\u03d5(s, ak) (25)\nThis Newton step can offer faster convergence, especially near the minimum, by adjusting the step direction\nand size using local curvature.\nWhile standard NNs often yield highly non-convex loss landscapes, PICNNs enforce convexity in action\nspace through architectural constraints (non-negative weights, convex activations), ensuring that updates\nreliably move toward a well-defined minimum. This makes the correction process stable, interpretable, and\nefficient which are key properties for deployment in industrial process control systems.\nAt each timestep during online deployment, the system observes the current state, queries the policy for\nan action, and refines this action using the gradient (or Newton) steps over the PICNN-modeled cost surface.\nThis safety layer operates without modifying the policy network and without requiring exploration, making\nit compatible with any offline-trained policy and suitable for constrained real-time settings.\nAlgorithm 1 Online Deployment with Action Correction via PICNN\n1:Inputs: Trained policy \u03c0, trained cost model \u02c6 c\u03d5(s, a), environment E, step size \u03b7\n2:Initialise state s0\n3:foreach timestep t= 0,1,2, . . .do\n4: Observe current state stfrom environment\n5: Compute proposed action at\u2190\u03c0(st)\n6: Compute gradient \u2207a\u02c6c\u03d5(st, at)\n7: ifNewton step enabled then\n8: Compute Hessian Ht\u2190 \u22072\na\u02c6c\u03d5(st, at)\n9: Correct action: at\u2190at\u2212H\u22121\nt\u2207a\u02c6c\u03d5(st, at)\n10: else\n11: Correct action: at\u2190at\u2212\u03b7\u00b7 \u2207a\u02c6c\u03d5(st, at)\n12: end if\n13: Optionally clip or project atto valid action bounds\n14: Execute action atin environment: st+1, rt\u2190 E(st, at)\n15:end for\n6 Results\n6.1 PICNN cost model analysis\nFigure 4 presents a comparison of predicted versus true costs for the grade change up scenario, evaluated\non a test set. The regular NN exhibits strong predictive accuracy, with a mean absolute error (MAE) of\n0.47 and a coefficient of determination (R2) of 0.99, closely tracking the ideal (i.e., predicted cost equals\ntrue cost) line. In contrast, the PICNN demonstrates a higher MAE of 2.20 and a notably lower R2of 0.73,\nindicating reduced accuracy in pointwise cost estimation. This performance gap is particularly pronounced\nfor samples with higher true cost values, where the PICNN tends to overpredict. The discrepancy reflects\nthe trade-off between flexibility and structure: while the PICNN imposes convexity with respect to actions,\nit also constrains model expressiveness, potentially limiting its fit to complex cost surfaces. Despite this\nlower regression accuracy, as shown in subsequent rollout experiments (Figure 5), the PICNN\u2019s structural\nbias can still translate to more reliable online control through stable gradient corrections. This highlights\nthe tradeoff between enforcing convexity constraints and achieving optimal predictive accuracy.\nFigure 5 shows the rollout cost comparison over time for the grade change up scenario, where an IQL agent\nperforms online control using gradient-based corrections informed by a learned cost model. The comparison\n13\n\n--- Page 14 ---\nFigure 4: Comparison of standard NN and PICNN cost model predictions on training data from the poly-\nmerisation CSTR grade change up scenario. The PICNN trades predictive performance for convexity.\nis between the regular NN (dashed blue line) and the PICNN (solid orange line). Shaded regions indicate\ntwo standard deviations from the mean cost over 100 rollouts. While both models initially yield similar\ncost trajectories, the PICNN leads to more consistent and stable cost reduction over time, particularly at\nlower costs. This improved convergence behavior highlights the benefit of action convexity in the cost model,\nenabling more reliable gradient-based corrections during online deployment.\nThese results highlight a key insight: accurate cost prediction does not guarantee effective control. The\nPICNN, though less precise in estimating absolute cost values (Figure 4), provides more consistent and\nreliable gradients for online optimisation (Figure 5), likely due to its convexity constraint in the action\nspace. This structural bias appears to regularise the gradient landscape, enabling the agent to more effectively\ndescend toward low-cost trajectories, especially over longer horizons.\nFigure 5: Comparison of environment cost feedback during rollouts of the polymerisation CSTR grade change\nup scenario controlled an IQL agent with online gradient step corrections on two cost models: a regular NN,\nand a PICNN. The line shows the mean cost, whilst the shaded area shows 2 standard deviations over 100\nrollouts.\nTo further investigate the observed difference in rollout performance between the PICNN and NN cost\nmodels, Figure 6 compares the gradients of the predicted cost with respect to each action dimension (initiator\nfeed, left, and coolant temperature, right) over the course of the rollouts. The regular NN produces gradients\nthat are initially small and relatively flat, but increase in magnitude and variability over time, especially for\n14\n\n--- Page 15 ---\nthe coolant temperature. This growing instability may lead to overly aggressive or misdirected corrections\nlater in the rollout. In contrast, the PICNN exhibits a more structured and bounded gradient profile. For the\ninitiator feed, the PICNN responds quickly with strong early gradients, followed by a steady stabilisation.\nFor the coolant temperature, the PICNN gradients remain consistently negative and smoother throughout,\nproviding more reliable directional information for control.\nFigure 6: Comparison of gradients during rollouts of the polymerisation CSTR grade change up scenario\ncontrolled by a BC agent with online gradient step corrections on two cost models: a regular NN, and a\nPICNN. Left: cost gradient with respect to initiator feed action. Right: cost gradient with respect to coolant\ntemperature action. The line shows the mean gradient magnitude, whilst the shaded area shows 2 standard\ndeviations over 100 rollouts.\nInterestingly, the gradients predicted by the two models for coolant temperature not only differ in magni-\ntude but often point in opposite directions: PICNN consistently suggests decreasing the coolant temperature,\nwhile the Regular NN trends upward. This directional disagreement implies a fundamental mismatch in the\noptimisation signals, which can lead to divergent behaviours during control. Moreover, the NN gradients are\ngenerally smaller in magnitude across both action dimensions, indicating a flatter cost surface with respect\nto the actions. This may limit the effectiveness of gradient-based action refinement, as updates derived from\nsuch weak gradients result in negligible policy improvement. Despite the NN achieving a better overall cost\nprediction fit (Figure 4), this flatness suggests that the model\u2019s cost surface may be overly smooth or under-\nsensitive to action changes. This, in turn, hints at the NN relying more heavily on its modelling capacity\nwith respect to state features rather than action sensitivity.\nBy contrast, the PICNN\u2019s convexity constraint limits its expressiveness in fitting arbitrary cost functions,\nbut encourages stronger, more informative gradients with respect to actions. This structural bias may shift\nsome modelling flexibility into the state representation, effectively balancing action sensitivity and state\nexpressiveness. The result is a more useful gradient field for online control even if it comes at the cost\nof slightly reduced prediction accuracy. These differences support the hypothesis that structural inductive\nbiases like convexity can improve the utility of learned models in control settings by promoting more stable,\ninterpretable, and effective gradients over the course of long-horizon decision making.\n6.2 Control performance\n6.2.1 Dataset generation\nFigure 7 illustrates the closed-loop behaviour of the polymerisation CSTR during the upward grade transition\nscenario. This scenario proved to be the most difficult of the 3 CSTR scenarios and so the results are\npresented here whilst the other scenario results are demonstrated in Sections A and B. The dataset was\ngenerated using a population of PI controllers, each with randomly sampled tuning parameters to encourage\ndiversity in trajectory behaviors. This approach simulates the type of variability and suboptimality that is\ncommon in industrial process operations, providing a realistic and heterogeneous dataset suitable for offline\nreinforcement learning.\nThe top-left panel shows the evolution of polymer concentration over time, with the desired setpoint\nindicated by a dashed black line. The system exhibits significant initial undershoot followed by a slow\n15\n\n--- Page 16 ---\nFigure 7: 100 rollouts comprising the polymerisation CSTR grade up dataset.\nrecovery, with varying degrees of convergence quality across trajectories. The spread in final concentration\nvalues highlights the impact of differing PI gains, with some controllers achieving near-setpoint regulation\nwhile others remain below the target.\nThe top-right panel depicts the reactor temperature response. All trajectories exhibit an initial overshoot\nand gradual settling near the target temperature, reflecting the thermal inertia of the system and the influence\nof controller tuning on transient dynamics.\nIn the bottom-left panel, the initiator feed rate shows rapid ramp-up behaviour with clear differences\nin aggressiveness and steady-state values, again due to tuning variation. This reflects how control input\ndiversity is implicitly encoded into the training data through controller sampling.\nLastly, the bottom-right panel shows the coolant temperature, which similarly exhibits settling dynamics\nand trajectory variance. The consistency of temperature regulation across trajectories despite variation in\ncontrol actions suggests that the underlying dynamics are well-behaved but sensitive to control design.\nOverall, this dataset captures a broad spectrum of process responses and control actions, making it\nwell-suited for training data-driven cost models and evaluating policy improvement techniques in offline\nreinforcement learning settings.\n6.2.2 Behaviour cloning\nFigure 8 shows the closed-loop system behaviour under control by the BC policy trained on the diverse\nPI-generated dataset described earlier. While the agent is able to reproduce average-case control behaviour\nreasonably well, several key shortcomings are apparent, particularly in the polymer concentration dynamics.\nCompared to the original PI controlled dataset, the BC policy exhibits a consistent tendency toward\nsluggish correction of polymer concentration, as shown in the top-left panel. While most trajectories even-\ntually approach the setpoint, the rate of convergence is noticeably slower and the transient undershoot more\nprolonged. This reflects the policy\u2019s bias toward conservative control actions, likely stemming from the\ndominance of suboptimal (slow-reacting) behaviour in the training data. This skew is consequential, as it\nnegatively impacts the policy\u2019s cumulative reward and explains its low performance scores in Table 4.\nThe reactor temperature (top-right), initiator feed rate (bottom-left), and coolant temperature (bottom-\nright) dynamics are more consistent and display improved regularity compared to the training data. In\nparticular, the control inputs show reduced variability, indicating that the BC agent has learned a smoothed\n16\n\n--- Page 17 ---\nFigure 8: BC agent rollouts on the grade up scenario.\npolicy that largely avoids erratic behaviour. However, this smoothing effect comes at the cost of reac-\ntivity, especially in the polymer concentration loop where timely intervention is critical for grade change\nperformance.\nOverall, while the BC policy captures the average tendencies of the training data, it lacks the adaptability\nrequired for faster or more precise responses. This highlights a key limitation of purely supervised offline\npolicies in process control settings where asymmetric control quality in the dataset can propagate directly\ninto suboptimal closed-loop behavior.\n6.2.3 BC with online gradient-based action corrections\nFigure 9 presents the closed-loop performance of the same BC agent augmented with online gradient-based\naction corrections using a PICNN cost model. This hybrid approach leverages the BC policy for baseline con-\ntrol actions, which are then refined in real time via gradient steps that minimise the learned cost, effectively\nsteering the system toward safer process control.\nThe most notable improvement is observed in the polymer concentration (top-left). Compared to Figure\n8, the sluggishness seen under pure BC control has been largely eliminated. Trajectories now exhibit faster\nconvergence to the setpoint, with significantly reduced steady-state error. In many cases, the system reaches\nthe target concentration within the first 40 hours showing a substantial improvement in responsiveness. This\ncomes at the cost of some overshoot in early transients, a side effect of the more aggressive corrections applied\nvia the gradient-based updates. It was important to tune the step size of the gradient descent to balance\nresponsiveness and stability, as too large a step could lead to larger overshoots and instability.\nA similar pattern is visible in the reactor temperature (top-right), which now exhibits sharper initial\ntransients but improved settling behavior overall. Importantly, the system remains within safe operational\nbounds across all trajectories, demonstrating the stability and safety-preserving nature of the cost model\u2019s\nconvex structure.\nThe control inputs, initiator feed (bottom-left) and coolant temperature (bottom-right), reflect this\nincreased actuation agility. The initiator feed trajectories ramp up more rapidly and saturate closer to\nthe nominal upper limit, suggesting that the cost model guides the corrections toward energetically efficient,\nyet assertive control adjustments. Coolant temperature responses show slightly more variability than under\npure BC control but remain stable and well-regulated.\n17\n\n--- Page 18 ---\nFigure 9: BC agent with online gradient-based action corrections on the grade up scenario.\nOverall, the addition of PICNN-based gradient corrections transforms the BC agent\u2019s behavior from\npassively average to actively competent with a 14 \u00d7score increase (Table 4). While overshoots are introduced\nin the pursuit of faster setpoint tracking, the tradeoff appears favourable, especially in light of the consistent\nsafety and marked performance gains. This result underscores the utility of convex cost models not only for\noffline policy evaluation, but also for safe and targeted online policy refinement.\n6.2.4 IQL agent performance\nFigure 10 presents the closed-loop performance of an IQL policy deployed in the same grade-up scenario.\nAmong all controllers evaluated so far, this pure offline RL approach yields the most consistent and perfor-\nmant behavior, setting a clear benchmark for comparison.\nThe polymer concentration trajectories (top-left) demonstrate rapid and reliable convergence to the set-\npoint with minimal overshoot and negligible steady-state error. Unlike the BC and BC+ variants, IQL\nachieves both responsiveness and precision without compromising stability. The variability between tra-\njectories is markedly reduced, indicating strong policy generalisation across initial conditions and process\ndisturbances.\nReactor temperature (top-right) regulation is equally impressive. All trajectories settle close to the\nsetpoint with minimal oscillation or noise amplification, suggesting that the learned value function effectively\nencoded not only the target behavior but also the cost of unnecessary actuation. The control effort is smooth\nand well-coordinated.\nThe initiator feed (bottom-left) and coolant temperature (bottom-right) profiles further underscore the\nquality of the policy. Feed trajectories rise quickly and saturate efficiently, with minimal trajectory spread.\nCoolant behavior shows tight regulation, minimal excursions, and no signs of instability, even during transient\nphases.\nCompared to the baseline PI rollouts, BC policy, and BC+ corrected actions, IQL clearly dominates\nin both control quality and consistency. This result validates IQL as a strong candidate for real-world\ndeployment, showcasing its ability to extract high-quality policies from offline data without requiring online\ncorrection or cost shaping. It serves as a practical gold standard for offline RL in process control settings\nwhere safety, sample efficiency, and asymptotic performance are all critical.\n18\n\n--- Page 19 ---\nFigure 10: IQL agent rollouts on the grade up scenario.\n6.2.5 IQL with online gradient-based action corrections\nFigure 11 shows the results of augmenting the IQL policy with online gradient-based action corrections using\nthe PICNN cost model. As seen, the performance remains largely on par with the uncorrected IQL controller,\nwhich already served as a strong baseline. However, subtle but meaningful refinements are apparent.\nThe most notable improvement lies in the polymer concentration trajectories (top-left). While IQL\nalready exhibited excellent setpoint tracking, a small subset of trajectories previously lagged slightly in con-\nvergence. With the addition of PICNN-based corrections, these sluggish responses are effectively eliminated.\nThe result is an even tighter clustering of trajectories around the setpoint, further reducing variance and\nimproving worst-case performance without compromising stability.\nIn the reactor temperature (top-right), initiator feed (bottom-left), and coolant temperature (bottom-\nright) plots, the differences compared to standard IQL are minimal. This is expected as IQL already\nachieves near-optimal actuation patterns in these variables. Importantly, the cost-based corrections do not\ndegrade performance or introduce instability, demonstrating the compatibility of PICNN refinements with\nhigh-quality base policies.\nOverall, while there is little headroom to improve upon IQL\u2019s already strong performance, applying\nonline corrections via a convex cost model provides an additional layer of robustness and fine-tuning. It\nacts as a safeguard against occasional suboptimal actions and offers a principled mechanism for enhancing\npolicy behavior in edge cases, particularly in safety-critical or highly sensitive control loops like polymer\nconcentration.\n6.2.6 Grade up scenario summary\nFigure 12 summarises the total episode reward distributions across all controllers, normalised such that the\nworst-performing trajectory in the PI-controlled dataset scores 0 and the best scores 100. This provides a\nconsistent benchmark for evaluating learning-based controllers relative to the diversity of behaviours present\nin the offline data.\nThe baseline dataset (Data) shows a wide spread in performance, reflecting the highly variable quality of\nthe random PI rollouts. The IQR spans nearly the full reward spectrum, with many low-quality trajectories\npulling down the median score to around 40. This highlights the challenging nature of learning from such\nnoisy, heterogeneous data.\n19\n\n--- Page 20 ---\nFigure 11: IQL agent with online gradient-based action corrections on the grade up scenario.\nBC performs poorly in this metric, with both mean and median rewards significantly below those of\nthe dataset. The narrow spread and dense cluster of low scores indicate that BC frequently reproduces\nthe conservative, sluggish behaviors present in the lower end of the data distribution as already seen in the\npolymer concentration responses (Figure 8).\nOnce the BC policy is augmented with PICNN-based gradient corrections (BC+), there is a dramatic\nimprovement. The median reward jumps substantially, approaching the upper quartile of the original dataset.\nThe variance is also reduced relative to the data, and notably, the worst-case performance improves by a\nlarge margin. This demonstrates that cost model corrections are highly effective in recovering much of the\nlost performance from a weak prior, transforming a non-viable offline RL agent into one that can compete\nin the upper half of the training data distribution.\nIQL already achieves high reward scores with low variance, confirming its strong and consistent policy\nperformance observed earlier (Figure 10). The range of episode rewards is tightly concentrated near the\nupper bound, indicating both efficiency and robustness across runs.\nApplying online corrections to IQL (IQL+) provides an additional lift in the reward distribution. While\nthe median remains nearly unchanged, both the minimum and maximum scores improve. This suggests that\nPICNN corrections can still add measurable benefit even on top of a strong offline RL policy, particularly by\nmitigating rare but costly suboptimal decisions. The PICNN-based action corrections significantly enhance\nBC policies and provide measurable robustness gains even for strong RL agents like IQL.\n20\n\n--- Page 21 ---\nFigure 12: Total episode rewards for rollouts on the polymerisation CSTR grade up scenario. The boxplots\nshow the distribution of rewards achieved by the PI controllers (Data), BC, IQL agents, and BC and IQL\nwith online gradient-based action corrections (BC+ and IQL+) across 100 episodes. The median is shown\nas a central line, the mean is shown by a diamond marker, the box represents the IQR of the data, and the\nwhiskers extend to the farthest data point lying within 1.5x the IQR from the box. Outliers beyond the end\nof the whiskers are shown as individual circle markers.\n21\n\n--- Page 22 ---\n6.3 Other scenarios\nTable 4 summarises the mean total rewards achieved by the different agents across all 3 polymerisation\nCSTR scenarios: startup, grade-down, and grade-up. All results are normalised relative to the PI-generated\ndatasets for each scenario, with 0 corresponding to the worst and 100 to the best PI trajectory. This allows\nfor meaningful comparisons of control quality across different operating regimes.\nTable 4: Summary of mean total rewards (normalised on the PI-generated datasets) from the polymerisation\nCSTR control scenarios.\nStartup Grade Down Grade Up Mean\nData 39.4 52.6 41.4 44.5\nBC 41.2 72.6 4.70 39.5\nBC+ 66.2 80.8 66.9 71.3\nIQL 98.6 89.0 88.4 92.0\nIQL+ 96.7 90.4 88.9 92.3\nIn the startup scenario, the IQL agent achieved the highest mean total reward (98.6), demonstrating\nexceptional performance. Interestingly, the IQL+ agent (which includes online gradient-based action correc-\ntions using a PICNN cost model) achieved a slightly lower mean score (96.7). However, the minimum total\nreward for IQL+ was notably higher (81.1 compared to 72.7), indicating improved consistency and robust-\nness. This suggests that while online corrections may not always raise average performance when starting\nfrom a strong policy, they can reduce worst-case deviations, which is particularly valuable in safety-critical\napplications.\nIn the grade-down scenario, both IQL and IQL+ again performed strongly, with IQL+ achieving the\nhighest mean reward (90.4), slightly outperforming IQL (89.0). The benefits of online correction are again\nseen in the tightened lower end of the performance distribution. Meanwhile, BC achieved surprisingly\nstrong results in this scenario (72.6), likely because the dynamics during grade-down transitions are more\nforgiving and less sensitive to timing errors. Still, BC+ provided a clear boost over BC (80.8 compared\nto 72.6), confirming that action refinement via gradient descent on a learned PICNN cost model improves\nperformance even when the base behaviour is relatively effective.\nIn the grade-up scenario, the performance differences were most pronounced. The BC agent performed\npoorly (4.7 mean reward), reflecting its inability to handle the faster dynamics and more aggressive control\nrequired. However, applying PICNN-based corrections (BC+) dramatically improved its mean performance\nto 66.9 (more than a 14 \u00d7increase) once again highlighting the utility of PICNN cost-driven refinement. IQL\nand IQL+ both achieved strong results here (88.4 and 88.9 respectively), with IQL+ slightly outperforming\non average.\nAcross all scenarios, the IQL+ agent attained the highest mean total reward overall (92.3), followed closely\nby IQL (92.0). These results support the view that offline RL is capable of producing high-performing and\ngeneralisable policies from diverse datasets. Moreover, the marginal yet consistent gains provided by IQL+\nshow that combining these policies with structure-preserving cost models enables more robust deployment,\nespecially under dynamic or uncertain conditions.\nIn contrast, the BC agent alone underperformed (overall mean 39.5), frequently replicating suboptimal\nbehavior from the dataset. Yet, when augmented with PICNN gradient corrections (BC+), its mean reward\nincreased dramatically to 71.3 reinforcing that PICNN cost model action corrections can rescue otherwise\nsuboptimal and potentially unsafe offline policies.\n7 Conclusion\nThis work demonstrates the feasibility and promise of applying offline RL to the safe and efficient control\nof chemical process systems, using a polymerisation CSTR as a representative case study. We developed\na realistic, Gymnasium-compatible simulation environment and generated diverse offline datasets using PI\ncontrollers, establishing a robust testbed for evaluating offline RL algorithms under industrially relevant\nconditions. BC served as a strong imitation baseline but was limited by its dependence on the quality\n22\n\n--- Page 23 ---\nand coverage of the offline data. In contrast, IQL leveraged value-based learning and advantage-weighted\nregression to outperform BC in terms of robustness and generalisation.\nTo address the safety and stability challenges inherent in deploying offline RL policies, we introduced\na practical and principled mechanism for real-time action correction using gradient-based updates over a\nlearned cost model. By structuring the cost function as a PICNN, we ensured convexity in the action space\nwhile retaining rich expressivity over system states. This architectural choice enables stable, interpretable,\nand efficient action refinement at deployment time without requiring additional environment interaction or\nexplicit constraint programming.\nEmpirical results show that enforcing convexity in the cost model leads to smoother optimisation land-\nscapes, more reliable corrective updates, and improved constraint satisfaction, all while maintaining task\nperformance. These results highlight the potential of offline RL as a data-efficient alternative to traditional\ncontrol strategies, particularly in complex transition scenarios where classical methods like PI control may\nrequire extensive tuning or fail to generalise.\nLooking forward, this work opens several promising research directions. Future efforts will focus on\nextending our method to multi-step or trajectory-based safety objectives, incorporating uncertainty esti-\nmation to improve robustness, and generalising convexity-aware correction to broader classes of value or\nreward models. Additionally, we are interested in exploring hybrid control architectures that combine the\ninterpretability and reliability of classical control with the flexibility and adaptability of learning-based ap-\nproaches. Integrating domain knowledge, safety constraints, and uncertainty quantification will be key to\nadvancing the safe deployment of RL in real-world industrial settings.\nAcknowledgements\nThe authors would like to acknowledge the financial support from Shell.\nReferences\n[1] R. S. Sutton and A. Barto, \u201cReinforcement Learning: An Introduction,\u201d 2018.\n[2] M. Baldea, A. T. Georgiou, B. Gopaluni, M. Mercang\u00a8 oz, C. C. Pantelides, K. Sheth, V. M. Zavala, and\nC. Georgakis, \u201cFrom automated to autonomous process operations,\u201d Computers & Chemical Engineer-\ning, vol. 196, p. 109064, 2025.\n[3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00b4 e, \u201cConcrete Problems in\nAI Safety,\u201d 2016. [Online]. Available: https://arxiv.org/abs/1606.06565\n[4] D. Bonvin, L. Bodizs, and B. Srinivasan, \u201cOptimal grade transition for polyethylene reactors via NCO\ntracking,\u201d Chemical Engineering Research and Design , vol. 83, no. 6, pp. 692\u2013697, 2005.\n[5] K. S. Lee and J. H. Lee, \u201cIterative learning control-based batch process control technique for integrated\ncontrol of end product properties and transient profiles of process variables,\u201d Journal of Process Control ,\nvol. 13, no. 7, pp. 607\u2013621, 2003.\n[6] A. Prata, J. Oldenburg, A. Kroll, and W. Marquardt, \u201cIntegrated scheduling and dynamic optimization\nof grade transitions for a continuous polymerization reactor,\u201d Computers & Chemical Engineering ,\nvol. 32, no. 3, pp. 463\u2013476, 2008.\n[7] A. M. Lyapunov, \u201cThe general problem of the stability of motion,\u201d International Journal of Control ,\nvol. 55, no. 3, pp. 531\u2013534, 1992.\n[8] H. K. Khalil and J. W. Grizzle, Nonlinear Systems , 3rd ed. Prentice Hall, 1996.\n[9] B. Amos, L. Xu, and J. Z. Kolter, \u201cInput Convex Neural Networks,\u201d 2017. [Online]. Available:\nhttps://arxiv.org/abs/1609.07152\n23\n\n--- Page 24 ---\n[10] F. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause, \u201cSafe Model-based Reinforcement\nLearning with Stability Guarantees,\u201d 2017. [Online]. Available: https://arxiv.org/abs/1705.08551\n[11] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, \u201cControl Barrier Function Based Quadratic Pro-\ngrams for Safety Critical Systems,\u201d IEEE Transactions on Automatic Control , vol. 62, no. 8, pp. 3861\u2013\n3876, 2017.\n[12] B. Gangopadhyay, P. Dasgupta, and S. Dey, \u201cSafe and Stable RL (S2RL) Driving Policies Using Control\nBarrier and Control Lyapunov Functions,\u201d IEEE Transactions on Intelligent Vehicles , vol. 8, no. 2, pp.\n1889\u20131899, 2023.\n[13] J. Achiam, D. Held, A. Tamar, and P. Abbeel, \u201cConstrained Policy Optimization,\u201d 2017. [Online].\nAvailable: https://arxiv.org/abs/1705.10528\n[14] G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa, \u201cSafe Exploration in\nContinuous Action Spaces,\u201d 2018. [Online]. Available: https://arxiv.org/abs/1801.08757\n[15] A. Kumar, A. Zhou, G. Tucker, and S. Levine, \u201cConservative Q-Learning for Offline Reinforcement\nLearning,\u201d 2020. [Online]. Available: https://arxiv.org/abs/2006.04779\n[16] I. Kostrikov, A. Nair, and S. Levine, \u201cOffline Reinforcement Learning with Implicit Q-Learning,\u201d 2021.\n[Online]. Available: https://arxiv.org/abs/2110.06169\n[17] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Zou, S. Levine, C. Finn, and T. Ma, \u201cMOPO: Model-based\nOffline Policy Optimization,\u201d 2020. [Online]. Available: https://arxiv.org/abs/2005.13239\n[18] J. Wang and M. Fazlyab, \u201cActor\u2013Critic Physics-Informed Neural Lyapunov Con-\ntrol,\u201d IEEE Control Systems Letters , vol. 8, p. 1751\u20131756, 2024. [Online]. Available:\nhttp://dx.doi.org/10.1109/LCSYS.2024.3416235\n[19] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, \u201cDistributional Reinforcement Learning\nwith Quantile Regression,\u201d 2017. [Online]. Available: https://arxiv.org/abs/1710.10044\n[20] D. Liu, Y. Wang, C. Liu, B. Luo, and B. Huang, \u201cEKG-AC: A New Paradigm for Process Indus-\ntrial Optimization Based on Offline Reinforcement Learning With Expert Knowledge Guidance,\u201d IEEE\nTransactions on Cybernetics , pp. 1\u201311, 2025.\n[21] Y. Chen, Y. Shi, and B. Zhang, \u201cOptimal Control Via Neural Networks: A Convex Approach,\u201d 2019.\n[Online]. Available: https://arxiv.org/abs/1805.11835\n[22] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter, \u201cDifferentiable Convex\nOptimization Layers,\u201d 2019. [Online]. Available: https://arxiv.org/abs/1910.12430\n[23] B. Amos and J. Z. Kolter, \u201cOptNet: Differentiable Optimization as a Layer in Neural Networks,\u201d\n2021. [Online]. Available: https://arxiv.org/abs/1703.00443\n[24] S. BenAmor, F. J. Doyle III, and R. McFarlane, \u201cPolymer grade transition control using advanced\nreal-time optimization software,\u201d Journal of Process Control , vol. 14, no. 4, pp. 349\u2013364, 2004.\n[25] Z.-F. Jiang, D. S.-H. Wong, J.-L. Kang, Y. Yao, and Y.-C. Chuang, \u201cPolymer grade transition control\nvia reinforcement learning trained with a physically consistent memory sequence-to-sequence digital\ntwin,\u201d in Computer Aided Chemical Engineering . Elsevier, 2023, vol. 52, pp. 297\u2013303.\n[26] D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. A. Runkler, and V. Sterzing, \u201cA benchmark\nenvironment motivated by industrial control problems,\u201d in 2017 IEEE Symposium Series on Computa-\ntional Intelligence (SSCI) , 2017, pp. 1\u20138.\n[27] A. Ray, J. Achiam, and D. Amodei, \u201cBenchmarking Safe Exploration in Deep Reinforcement\nLearning,\u201d 2019. [Online]. Available: https://arxiv.org/abs/1910.01708\n24\n\n--- Page 25 ---\n[28] M. Bloor, J. Torraca, I. O. Sandoval, A. Ahmed, M. White, M. Mercang\u00a8 oz, C. Tsay, E. A. D. R.\nChanona, and M. Mowbray, \u201cPC-Gym: Benchmark Environments For Process Control Problems,\u201d\narXiv preprint arXiv:2410.22093 , 2024.\n[29] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, \u201cEnd-to-End Safe Reinforcement Learning\nthrough Barrier Functions for Safety-Critical Continuous Control Tasks,\u201d 2019. [Online]. Available:\nhttps://arxiv.org/abs/1903.08792\n[30] T. Kim, H. Suk, and S. Kim, \u201cOffline reinforcement learning methods for real-world problems,\u201d in\nAdvances in Computers . Elsevier, 2024, vol. 134, pp. 285\u2013315.\n[31] S. Levine, A. Kumar, G. Tucker, and J. Fu, \u201cOffline reinforcement learning: Tutorial, review, and\nperspectives on open problems,\u201d arXiv preprint arXiv:2005.01643 , 2020.\n[32] R. F. Prudencio, M. R. Maximo, and E. L. Colombini, \u201cA survey on offline reinforcement learning:\nTaxonomy, review, and open problems,\u201d IEEE Transactions on Neural Networks and Learning Systems ,\n2023.\n[33] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine, \u201cStabilizing off-policy q-learning via bootstrapping\nerror reduction,\u201d Advances in neural information processing systems , vol. 32, 2019.\n[34] H. van Hasselt, A. Guez, and D. Silver, \u201cDeep Reinforcement Learning with Double Q-learning,\u201d 2015.\n[Online]. Available: https://arxiv.org/abs/1509.06461\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature ,\nvol. 518, no. 7540, pp. 529\u2013533, 2015.\n[36] Y. Wu, G. Tucker, and O. Nachum, \u201cBehavior Regularized Offline Reinforcement Learning,\u201d 2019.\n[Online]. Available: https://arxiv.org/abs/1911.11361\n[37] X. B. Peng, A. Kumar, G. Zhang, and S. Levine, \u201cAdvantage-Weighted Regression: Simple and Scalable\nOff-Policy Reinforcement Learning,\u201d 2019. [Online]. Available: https://arxiv.org/abs/1910.00177\n[38] A. Ahmed, E. A. del Rio-Chanona, and M. Mercang\u00a8 oz, \u201cComparative Study of Machine Learning and\nSystem Identification for Process Systems Engineering Dynamics,\u201d Industrial & Engineering Chemistry\nResearch , 2025.\n[39] B. R. Maner and F. J. Doyle III, \u201cPolymerization reactor control using autoregressive-plus Volterra-\nbased MPC,\u201d AIChE Journal , vol. 43, no. 7, pp. 1763\u20131784, 1997.\n25\n\n--- Page 26 ---\nA Startup scenario results\nFigure 13: 100 PI controlled episodes from the polymerisation CSTR startup scenario.\n26\n\n--- Page 27 ---\nFigure 14: 100 BC episodes for the startup scenario.\nFigure 15: 100 IQL episodes for the startup scenario.\n27\n\n--- Page 28 ---\nFigure 16: 100 episodes for the startup scenario controlled by the BC agent with online gradient correction.\nFigure 17: 100 episodes for the startup scenario controlled by the IQL agent with online gradient correction.\n28\n\n--- Page 29 ---\nFigure 18: Boxplot of rewards for the startup scenario.\n29\n\n--- Page 30 ---\nB Grade down scenario results\nFigure 19: 100 PI controlled episodes from the polymerisation CSTR grade change down scenario.\n30\n\n--- Page 31 ---\nFigure 20: 100 BC episodes for the grade change down scenario.\nFigure 21: 100 IQL episodes for the grade change down scenario.\n31\n\n--- Page 32 ---\nFigure 22: 100 episodes for the grade change down scenario controlled by the BC agent with online gradient\ncorrection.\nFigure 23: 100 episodes for the grade change down scenario controlled by the IQL agent with online gradient\ncorrection.\n32\n\n--- Page 33 ---\nFigure 24: Boxplot of rewards for the grade change down scenario.\n33",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2507.22640v1_Safe_Deployment_of_Offline_Reinforcement_Learning_",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2507.22640v1_Safe_Deployment_of_Offline_Reinforcement_Learning_/.agent_comm",
  "assigned_at": "2025-07-31T21:03:20.111620",
  "status": "assigned"
}